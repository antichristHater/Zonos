{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the Zonos model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initing zonos for ft\n"
     ]
    }
   ],
   "source": [
    "from AudioInfo import Ses, SesFromArray\n",
    "import torch\n",
    "from zonos.model import Zonos\n",
    "from zonos.conditioning import make_cond_dict\n",
    "from zonos.codebook_pattern import apply_delay_pattern, revert_delay_pattern\n",
    "from huggingface_hub import hf_hub_download\n",
    "from zonos.sampling import sample_from_logits\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path=\"Zyphra/Zonos-v0.1-transformer\"\n",
    "device = 'cuda'\n",
    "\n",
    "model = Zonos.from_pretrained(model_path, device=device)\n",
    "cfg_scale = 2\n",
    "sampling_params = dict(min_p=0.1)\n",
    "cg = model.can_use_cudagraphs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Funcs to grab & preprocess input audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silero_vad import load_silero_vad, get_speech_timestamps\n",
    "silero_model = load_silero_vad()\n",
    "\n",
    "def audio_to_prefix_code(arr, sr):\n",
    "    wav_prefix, sr_prefix = torch.tensor(arr, dtype=torch.float32).unsqueeze(0), sr\n",
    "    wav_prefix = wav_prefix.mean(0, keepdim=True)\n",
    "    wav_prefix = model.autoencoder.preprocess(wav_prefix, sr_prefix)\n",
    "    wav_prefix = wav_prefix.to(device, dtype=torch.float32)\n",
    "    return model.autoencoder.encode(wav_prefix.unsqueeze(0))\n",
    "\n",
    "def ses_to_prefix_code(ses: Ses):\n",
    "    '''Generate codes from ses (audio).'''\n",
    "    if sum(torch.tensor(ses.arr)) != 0:\n",
    "            return audio_to_prefix_code(ses.arr, ses.sr)\n",
    "    else:\n",
    "        print(\"passed empty prefix\")\n",
    "        return torch.full((1,9,0), 0).to(model.device)\n",
    "\n",
    "def get_stamps(ses:Ses):\n",
    "    '''Get the segments with human voice'''\n",
    "    if ses.sr != 16000:\n",
    "        ses = ses.resampled(16000)\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        torch.tensor(ses.arr, dtype=torch.float32),\n",
    "        silero_model,\n",
    "        return_seconds=True,  # Return speech timestamps in seconds (default is samples); change this with samples and convert to seconds, it is currently buggy.\n",
    "    )\n",
    "    if len(speech_timestamps) < 1: return False\n",
    "    return speech_timestamps[0]['start'], speech_timestamps[-1]['end']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prefill with the input audio codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prefix(codes: torch.Tensor, prefix_conditioning=None, trim:None|int=None):\n",
    "    '''Generate model-accurate prefices to train on from codes and conditioning'''\n",
    "    assert isinstance(prefix_conditioning, torch.Tensor)\n",
    "    if isinstance(codes, Ses):\n",
    "        codes = ses_to_prefix_code(codes) # if a `Ses` instance, process\n",
    "    assert isinstance(codes, torch.Tensor)\n",
    "    # Encode input audio\n",
    "    prefix_codes = codes\n",
    "    p_len = prefix_codes.size(-1)\n",
    "    # Hyperparams\n",
    "    max_length = 86 * 30\n",
    "    seq_len = p_len + max_length + 9\n",
    "    # Inference params\n",
    "    with torch.device(model.device):\n",
    "        batch_size__ = 1\n",
    "        unknown_token = -1\n",
    "        inference_params = model.setup_cache(batch_size=batch_size__ * 2, max_seqlen=seq_len)\n",
    "    # Inference mode (no gradients are needed)\n",
    "    with torch.no_grad():\n",
    "        # Prepare prefix codes\n",
    "        p_padded = torch.nn.functional.pad(prefix_codes, (0, max_length + p_len), value=unknown_token)\n",
    "        # Re-predict missing token\n",
    "        p_delayed = apply_delay_pattern(p_padded, mask_token=model.masked_token_id)\n",
    "        pred_idx = p_len if trim is None else trim\n",
    "        logits = model._prefill(prefix_conditioning,\n",
    "                                p_delayed[...,:pred_idx + 1],\n",
    "                                inference_params, 2)\n",
    "        next_token = sample_from_logits(logits, **sampling_params)\n",
    "\n",
    "        frame = p_delayed[..., pred_idx + 1:pred_idx + 2]\n",
    "        frame.masked_scatter_(frame == unknown_token, next_token)\n",
    "    # Offset and logit\n",
    "    offset = p_delayed[...,:pred_idx + 1].size(-1)\n",
    "    logit_bias = torch.zeros_like(logits)\n",
    "    logit_bias[:, 1:, model.eos_token_id] = -torch.inf\n",
    "    # Inference params\n",
    "    prefices_length = prefix_conditioning.shape[1] + pred_idx + 1\n",
    "    inference_params.seqlen_offset += prefices_length\n",
    "    inference_params.lengths_per_sample[:] += prefices_length\n",
    "    to_compare = p_delayed[...,offset+1:offset+2]\n",
    "    return p_delayed, inference_params, offset, logit_bias, to_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prepare prefix for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed empty prefix\n"
     ]
    }
   ],
   "source": [
    "new_condt = make_cond_dict(\n",
    "        text= \"Greetings, lad. What would your name be?\",\n",
    "        language='en-us',\n",
    "    )\n",
    "\n",
    "# To do audio completion\n",
    "ses_ = Ses('sample.wav')\n",
    "prefix_codes = ses_to_prefix_code(ses_)\n",
    "prefix_conditioning = model.prepare_conditioning(new_condt)\n",
    "\n",
    "# Empty input to generate audios from no prefices\n",
    "# If you don't have an input audio to start with and you want to generate from scratch, just use this empty tensor.\n",
    "empty_ses = SesFromArray(torch.tensor(()).to(torch.float64).numpy(), 6)\n",
    "with torch.no_grad():\n",
    "    delayed_codes, inference_params, offset, logit_bias, to_compare = prepare_prefix(empty_ses, prefix_conditioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generation loop / A second = 86 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 258/258 [00:21<00:00, 12.22it/s]\n"
     ]
    }
   ],
   "source": [
    "SECONDS_TO_GENERATE = 3\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(86*SECONDS_TO_GENERATE)):\n",
    "        # Increase offset / Offseti artır\n",
    "        offset += 1\n",
    "\n",
    "        # Calculate next logit / Sonraki logiti hesapla\n",
    "        input_ids = delayed_codes[..., offset - 1 : offset] # tensor([ 698, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025], device='cuda:0') [9, 1]; 698 next_tokenin 0. elemanıydı.\n",
    "        logits = model._decode_one_token(input_ids, inference_params, cfg_scale, allow_cudagraphs=cg) # torch.Size([1, 9, 1026]); \n",
    "        logits += logit_bias # decode_one_token'in son elementlerinde [1025ler] olasılık zaten -inf'di. \n",
    "        next_token = sample_from_logits(logits, generated_tokens=delayed_codes[..., :offset], **sampling_params)\n",
    "\n",
    "        # Append the new token / Yeni tokeni ekle\n",
    "        frame = delayed_codes[..., offset : offset + 1]\n",
    "        frame.masked_scatter_(frame == -1, next_token)\n",
    "\n",
    "        # Increase inference_params / Inference ayarla.\n",
    "        inference_params.seqlen_offset += 1\n",
    "        inference_params.lengths_per_sample[:] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 250])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Make this a function\n",
    "with torch.no_grad():\n",
    "    out_codes = revert_delay_pattern(delayed_codes)\n",
    "    out_codes.masked_fill_(out_codes >= 1024, 0)\n",
    "    out_codes = out_codes[..., : offset - 9]\n",
    "    print(out_codes.shape)\n",
    "    decodedarr = model.autoencoder.decode(out_codes).squeeze().to(torch.float64).cpu()\n",
    "    SesFromArray(decodedarr.numpy(), 44100).write('demo.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
